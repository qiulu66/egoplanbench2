<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios">
  <meta name="keywords" content="MLLMs, Benchmark, Human-level planning, Egocentric perspective">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="https://assets.crowd.aws/crowd-html-elements.js"></script> -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">






  <style>
    /* .section {
      padding: 3rem 1.5rem;
    }

    .container {
      max-width: 800px;
    }

    #video-container {
      margin-bottom: 2rem;
    }

    #question-container {
      margin-bottom: 2rem;
    }

    .buttons {
      display: flex;
      justify-content: space-between;
    }

    .button.is-primary {
      background-color: #3273dc;
      border-color: transparent;
      color: #fff;
      transition: background-color 0.3s ease;
    }

    .button.is-primary:hover {
      background-color: #275ab0;
      border-color: transparent;
    }

    .title img {
      margin-right: 0.5rem;
    }

    .select {
      width: 100%;
      margin-top: 1rem;
    } */

    /* Hide the "Previewing Answers Submitted by Workers" message */
    crowd-alert {
      display: none !important;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div style="display: flex; align-items: center; justify-content: center;">
            <!-- <img src="./static/image/worldevallogo.png" style="width:6em;vertical-align: middle" alt="Logo"/>  -->
            <h1 class="title is-1 publication-title" style="display: inline-block;">EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios</h1>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Lu Qiu</a><sup style="color: #FFB6C1;">1,2</sup>,</span>
            <span class="author-block">
              <a>Yuying Ge</a><sup style="color: #ADD8E6;">†,2</sup>,</span>
            <span class="author-block">
              <a>Yi Chen</a><sup style="color: #FFB6C1;">1,2</sup>,</span>
            <span class="author-block">
              <a>Yixiao Ge</a><sup style="color: #FFB6C1;">2</sup>,</span>
            <span class="author-block">
              <a>Ying Shan</a><sup style="color: #FFB6C1;">2</sup>,</span>
            <span class="author-block">
              <a>Xihui Liu</a><sup style="color: #ADD8E6;">†,1</sup>,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color: #FFB6C1;">1</sup>The University of Hong Kong, Hong Kong,</span>
            <span class="author-block"><sup style="color: #ADD8E6;">2</sup>ARC Lab, Tencent PCG, Shenzhen,</span>
          </div>
          <!-- <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#e08ba0; font-weight:normal"> <b>Under Review</b> </b></span>
          </div> -->
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">†Corresponding to:</span>
            <span class="author-block"><a href="mailto:yyge13@gmail.com">yyge13@gmail.com</a>,</span>
            <span class="author-block"><a href="mailto:xihuiliu@eee.hku.hk">xihuiliu@eee.hku.hk</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->

              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.04447/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/qiulu66/EgoPlan-Bench2/tree/main/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>dataset</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/qiulu66/EgoPlan-Bench2/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="overview" width="150%" src="./static/Images/teaser.png">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>Figure 1. Left: EgoPlan-Bench2 encompasses planning tasks spanning four major domains and 24 detailed scenarios for evaluating the planning capabilities of MLLMs in diverse real-world contexts. Right: Examples of our multiple-choice question-answer pairs, where a partial video showing historical task progress, a current observation image, and a task goal expressed in language are given for a model to select the most appropriate action. </b></p>
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The advent of Multimodal Large Language Models (MLLMs), leveraging the power of Large Language Models, has recently demonstrated superior multimodal understanding and reasoning abilities, heralding a new era for artificial general intelligence (AGI). However, achieving AGI necessitates more than just comprehension and reasoning. A crucial capability required is effective planning in diverse scenarios, which involves making reasonable decisions based on complex environments to solve real-world problems. Despite its importance, the planning abilities of current MLLMs in varied scenarios remain underexplored, leaving a significant gap in our understanding of their full potential.
          </p>
          <p>
            In this paper, we introduce <b>EgoPlan-Bench2</b>, a rigorous and comprehensive benchmark designed to assess the planning capabilities of MLLMs across a wide range of real-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4 major domains and 24 detailed scenarios, closely aligned with human daily life. EgoPlan-Bench2 is constructed through a semi-automatic process utilizing egocentric videos, complemented by manual verification. Grounded in a first-person perspective, it mirrors the way humans approach problem-solving in everyday life. We evaluate 21 competitive MLLMs and provide an in-depth analysis of their limitations, revealing that they face significant challenges in real-world planning. To further improve the planning proficiency of current MLLMs, we propose a training-free approach using multimodal Chain-of-Thought (CoT) prompting through investigating the effectiveness of various multimodal prompts in complex planning. Our approach enhances the performance of GPT-4V by 10.24% on EgoPlan-Bench2 without additional training. Our work not only sheds light on the current limitations of MLLMs in planning, but also provides insights for future enhancements in this critical area. 
          </p>
          <p>
            This repository describes the usage of our proposed EgoPlan-Bench2, and provides the corresponding codes for benchmarking MLLMs and enhancing GPT-4V's performance by multimodal CoT prompting. Welcome to evaluate your models and explore methods to enhance the models' EgoPlan capabilities on our benchmark!
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Dataset Statistics </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="dataset" width="100%" src="./static/Images/data_statistic.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 2. Left: Scenarios distribution of EgoPlan-Bench2, which covers 4 major domains and 24 fine-grained scenarios. Right: Video length distribution. Our benchmark has a full spectrum of video duration, ranging from a few seconds to five minutes. </b></p>
            </h3>   
        </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Study on MLLMs Performance </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model_performance_table" width="100%" src="./static/Images/model_performance_new.jpg">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 3. Models' performance across different scenarios and video lengths. </b></p>
            </h3>
            <img id="model_performance_scenarios" width="100%" src="./static/Images/results_24scenarios.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 4. The accuracy of 21 MLLMs across the 4 main domains in human life. </b></p>
            </h3>   
        </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Enhancing Human-Level Planning by Prompting </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model_performance_table" width="100%" src="./static/Images/cot_new.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 5. The pipeline of our training-free multimodal Chain-of-Thought (CoT) prompting method. We utilize predicted actions sequences as a prompt for representing historical task progress, and bounding box of key objects as a prompt to enhance the understanding of visual observations. By combining these elements with CoT reasoning and a self-consistency mechanism, we strengthen GPT-4V's planning capabilities without the need for additional training. </b></p>
            </h3>
        </div>
  </div>
</section>






<!-- <section class="section" id="examples">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Examples</h2>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <div id="video-container">
          <video id="example-video" width="100%" height="315" controls>
            <source id="video-source" src="" type="video/mp4">
          </video>
        </div>

        <div id="question-container">
          <p id="question"></p>
          <select id="options" required=""></select>
        </div>

        <div class="buttons">
          <button id="prev-button" class="button is-primary">Previous</button>
          <button id="next-button" class="button is-primary">Next</button>
        </div>
      </div>
    </div>
  </div>
</section> -->




<section class="section" id="examples">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">
          <img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png" alt="Painting Icon">
          EgoPlan-Bench2 Examples
        </h2>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div id="video-container" class="box">
          <video id="example-video" class="is-fullwidth" controls>
            <source id="video-source" src="" type="video/mp4">
          </video>

          <p class="has-text-weight-bold" style="color: #4682b4;">Question: <span id="question" style="color: #4682b4;"></span></p>
          <p class="has-text-weight-bold" style="color: #ff69b4;">Choices: <span id="choices" style="color: #ff69b4;"></span></p>
          <p class="has-text-weight-bold" style="color: #66cdaa;">Ground Truth: <span id="gt" style="color: #66cdaa;"></span></p>
          <div class="select is-fullwidth" style="margin-top: 1rem;">
            <select name="video-question-answer" id="options" required="">
            </select>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{qiu2024egoplanbench2,
        title   = {EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios},
        author  = {Qiu, Lu and Ge, Yuying and Chen, Yi and Ge, Yixiao and Shan, Ying and Liu, Xihui},
        year    = {2024},
        journal = {arXiv preprint arXiv:2412.04447}
      }
    </code></pre>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a rel="license"
            href="https://github.com/MMWorld-bench/MMWorld-bench.github.io/">MMWorld-bench</a>, licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>

</footer>





<!--script>
  let data = [];
  let currentIndex = 0;
  const videoUrlPrefix = "https://videounderstanding.s3.us-east-2.amazonaws.com/";

  function loadCSV() {
    $.ajax({
      type: "GET",
      url: "./static/webdemo_data.csv",
      dataType: "text",
      success: function(response) {
        data = $.csv.toObjects(response);
        updateContent();
      }
    });
  }

  function updateContent() {
    if (data.length > 0 && currentIndex >= 0 && currentIndex < data.length) {
      const currentData = data[currentIndex];
      const videoUrl = videoUrlPrefix + currentData.video_url;
      $('#video-source').attr('src', videoUrl);
      $('#example-video')[0].load();
      $('#question').text(currentData.question);
      $('#discipline').text(currentData.domain);
      $('#subdiscipline').text(currentData.subdomain);
      const options = [currentData.option_a, currentData.option_b, currentData.option_c, currentData.option_d];
      $('#options').empty();
      options.forEach(option => {
        $('#options').append(`<option value="${option}">${option}</option>`);
      });
    }
  }

  $('#next-button').on('click', function() {
    if (currentIndex < data.length - 1) {
      currentIndex++;
      updateContent();
    }
  });

  $('#prev-button').on('click', function() {
    if (currentIndex > 0) {
      currentIndex--;
      updateContent();
    }
  });

  $(document).ready(function() {
    loadCSV();
    
    const observer = new MutationObserver(() => {
      const alertElement = document.querySelector('crowd-form').shadowRoot.querySelector('crowd-alert').shadowRoot.querySelector('awsui-alert');
      if (alertElement) {
        alertElement.style.display = 'none';
      }
    });
    
    observer.observe(document.querySelector('crowd-form').shadowRoot, { childList: true, subtree: true });
  });
</script -->

<script>
  let data = [];
  const videoUrlPrefix = "https://videounderstanding.s3.us-east-2.amazonaws.com/";

  function loadCSV() {
    $.ajax({
      type: "GET",
      url: "./static/Videos/examples.csv",
      dataType: "text",
      success: function(response) {
        data = $.csv.toObjects(response);  // 将 CSV 转换为对象数组
        populateSelectOptions();
      }
    });
  }

  function populateSelectOptions() {
    if (data.length > 0) {
      const $select = $('#options');
      $select.empty();

      // 根据CSV文件内容填充选择项
      data.forEach((item, index) => {
        const optionText = item.select;  // 显示CSV中的select列内容
        $select.append(`<option value="${index}">${optionText}</option>`);
      });

      // 设置更改事件
      $select.on('change', function() {
        const selectedIndex = parseInt($(this).val(), 10);
        updateContent(selectedIndex);
      });

      // 加载初始内容
      updateContent(0);
    }
  }

  function updateContent(index) {
    if (data.length > 0 && index >= 0 && index < data.length) {
      const currentData = data[index];
      const videoUrl = currentData["video_path"];
      $('#video-source').attr('src', videoUrl);
      $('#example-video')[0].load();
      $('#question').text(currentData.question);
      $('#choices').text(currentData.options);
      $('#gt').text(currentData.label);
    }
  }

  $(document).ready(function() {
    loadCSV();
    
    const observer = new MutationObserver(() => {
      const alertElement = document.querySelector('crowd-form').shadowRoot.querySelector('crowd-alert').shadowRoot.querySelector('awsui-alert');
      if (alertElement) {
        alertElement.style.display = 'none';
      }
    });
    
    observer.observe(document.querySelector('crowd-form').shadowRoot, { childList: true, subtree: true });
  });
</script>


<!-- Additional libraries for parsing CSV -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-csv/1.0.21/jquery.csv.min.js"></script>

</body>
</html>
